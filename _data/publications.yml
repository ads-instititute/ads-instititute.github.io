- year: 2019
  groups:
  - name: Papers
    papers:
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Active strict saddles in nonsmooth optimization
      journal: arxiv abs/1912.07146
      url: https://arxiv.org/abs/1912.07146
      highlight: We identify a strict-saddle property for nonsmooth functions that guarantees that typical algorithms (proximal gradient, proximal point, and prox-linear) only converge to local minimizers, when randomly initialized. The new strict saddle property combines negative curvature with existence of an active manifold, and provably holds for generic semi-algebraic problems.

    - author: Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, Junyu Zhang
      title: From low probability to high confidence in stochastic convex optimization
      journal: arxiv abs/1907.13307
      url: https://arxiv.org/abs/1907.13307
      highlight: We develop a generic procedure that augments a wide class of stochastic algorithms with high confidence guarantees at an overhead cost that is only logarithmic in the confidence level and polylogarithmic in the condition number. We discuss consequences for both streaming (online) algorithms and offline algorithms based on empirical risk minimization.
    
    - author: Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Díaz, Lijun Ding, Dmitriy Drusvyatskiy
      title: Low-rank matrix recovery with composite optimization : good conditioning and rapid convergence
      journal: arxiv abs/1904.10020
      url: https://arxiv.org/abs/1904.10020
      highlight: We show that for a variety of low-rank matrix recovery tasks, nonsmooth problem formulations are automatically well-conditioned, thereby endowing standard numerical methods with optimal sample and computation efficiency guarantees.                                                                                                                                                                   
      
    - author: Sébastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, Aaron Sidford
      title: Complexity of Highly Parallel Non-Smooth Convex Optimization
      journal: arxiv abs/1906.10655
      url: https://arxiv.org/abs/1906.10655
      highlight: We give a parallel optimization algorithm for non-smooth convex functions and show that this has the optimal number of rounds. Our lower bound improves upon a decades-old construction by Nemirovski.

    - author: Ruoqi Shen, Yin Tat Lee
      title: The Randomized Midpoint Method for Log-Concave Sampling
      journal: arxiv abs/1909.05503
      url: https://arxiv.org/abs/1909.05503
      highlight: We propose a new framework to discretize stochastic differential equations and give a much faster sampling algorithm for well-conditioned log-concave distributions.

    - author: Yue Sun, Nicolas Flammarion, Maryam Fazel
      title: Escaping from saddle points on Riemannian manifolds
      journal: arXiv abs/1906.07355
      url: https://arxiv.org/abs/1906.07355
      highlight: We show that perturbed Riemannian gradient descent on Riemannian manifolds escapes from saddle points and converges to an approximate local minimum.
      
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Uniform graphical convergence of subgradients in nonconvex optimization and learning
      journal: arXiv abs/1810.07590
      url: https://arxiv.org/abs/1810.07590
      highlight: We establish finite-sample bounds on uniform subgradient estimation of weakly convex functions through sampling.                                                                                                                                                                               
      
    
    
- year: 2018
  groups:
  - name: Papers
    papers:
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Stochastic model-based minimization of weakly convex functions
      journal: arXiv abs/1803.06523
      url: https://arxiv.org/abs/1803.06523
      highlight: This work proves a convergence rate of \(O(k^{-1/4})\) for a wide class of algorithms that are based on sampling one sided models of the function. Primary examples are the stochastic subgradient method and the stochastic prox-linear algorithm for minimizing a composition of a convex function with a smooth map.
      
    - author: Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, Jason D. Lee
      title: Stochastic subgradient method converges on tame functions
      journal: arxiv abs/1804.07795
      url: https://arxiv.org/abs/1804.07795
      highlight: We prove that the stochastic subgradient method, on any ``Whitney stratifiable'' locally Lipschitz function, produces limit points that are all first-order stationary. The class of Whitney stratifiable functions is virtually exhaustive in data science, and in particular, includes all popular deep learning architectures.
   
    - author: Maryam Fazel, Rong Ge, Sham M. Kakade, Mehran Mesbahi
      title: Global Convergence of Policy Gradient Methods for Linearized Control Problems
      journal: arXiv abs/1801.05039
      url: https://arxiv.org/abs/1801.05039
      highlight: This work shows that (model free) policy gradient methods globally converge to the optimal solution and are efficient with regards to their sample and computational complexities.
      
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Stochastic subgradient method converges at the rate \(O(k^{−1/4})\) on weakly convex functions
      journal: arXiv abs/1802.02988
      url: https://arxiv.org/abs/1802.02988
      highlight: This work proves that the proximal stochastic subgradient method converges at a rate \(O(k^{-1/4})\) on weakly convex problems. In particular, it resolves the long-standing open question on the rate of convergence of the proximal stochastic gradient method (without batching) for minimizing a sum of a smooth function and a proximable convex function.
      
    - author: Reza Eghbali, James Saunderson, Maryam Fazel
      title: Competitive Online Algorithms for Resource Allocation over the Positive Semidefinite Cone
      journal: arXiv abs/1802.01312
      url: https://arxiv.org/abs/1802.01312
      highlight: Competitive Online Algorithms for Resource Allocation over the Positive Semidefinite Cone
      
    - author: Dmitriy Drusvyatskiy, Maryam Fazel, Scott Roy
      title: An optimal first order method based on optimal quadratic averaging
      journal: arXiv abs/1604.06543
      url: https://arxiv.org/abs/1604.06543
      highlight: An optimal first order method based on optimal quadratic averaging
      
    - author: Amin Jalali, Maryam Fazel, Lin Xiao
      title: Variational Gram Functions- Convex Analysis and Optimization
      journal: arXiv abs/1507.04734
      url: https://arxiv.org/abs/1507.04734
      highlight: This paper considers a new class of convex penalty functions, called variational Gram functions (VGFs), that can promote pairwise relations, such as orthogonality, among a set of vectors in a vector space.

- year: 2017
  groups:
  - name: Survey
    papers:
      - author: Dmitriy Drusvyatskiy, Henry Wolkowicz
        title: The Many Faces of Degeneracy in Conic Optimization
        journal: Foundations and Trends in Optimization, Vol. 3, No. 2, pp 77-170, 2017.
        url: http://www.nowpublishers.com/article/Details/OPT-011
        highlight: This work surveys the roles of Slater's condition and the facial reduction technique in conic optimization.
      
      - author: Dmitriy Drusvyatskiy
        title: The proximal point method revisited
        journal: Survey submitted to SIAG/OPT Views and News
        url: https://arxiv.org/abs/1712.06038
        highlight: This short survey revisits the role of the proximal point method in large scale optimization, focusing on three recent examples&#58; a proximally guided stochastic subgradient method, the prox-linear algorithm, and Catalyst generic acceleration.

  - name: Papers
    papers:
      - author: Hongzhou Lin, Julien Mairal, Zaid Harchaoui
        title: Catalyst Acceleration for First-order Convex Optimization&#58; from Theory to Practice
        journal: arXiv abs/1712.05654
        url: https://arxiv.org/abs/1712.05654
        highlight: The paper presents the comprehensive theory and practice of the Catalyst acceleration scheme for accelerating gradient-based convex optimization methods.<br> Catalyst applies to a large class of optimization algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.
      
      - author: John Thickstun, Zaid Harchaoui, Dean Foster, Sham M. Kakade
        title: Invariances and Data Augmentation for Supervised Music Transcription
        journal: arXiv abs/1711.04845
        url: https://arxiv.org/abs/1711.04845
        highlight: This paper presents the winning entry to the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. The method is based on a translation-invariant convolutional architecture.
      
      - author: Damek Davis, Dmitriy Drusvyatskiy, Courtney Paquette
        title: The nonsmooth landscape of phase retrieval
        journal: arXiv abs/1711.03247
        url: https://arxiv.org/abs/1711.03247
        highlight: This paper shows that the standard Polyak subgradient method converges linearly for the phase retrieval problem, when initialized within a constant relative distance of the underlying signal. The second part of the paper analyzes concentration of stationary points of the subsampled objective.
      
      - author: Sébastien Bubeck, Michael B. Cohen, Yin Tat Lee, Yuanzhi Li
        title: An homotopy method for \(\ell_p\) regression provably beyond self-concordance and in input-sparsity time
        journal: arXiv abs/1711.01328
        url: http://arxiv.org/abs/1711.01328
        highlight: Solving two long open problems about $\ell_p$ ball&#58; <br> 1. The existence of input sparsity time algorithm for lp regression. <br> 2. The inexistence of good self-concordance barrier.
        
      - author: Sébastien Bubeck, Michael B. Cohen, James R. Lee, Yin Tat Lee, Aleksander Madry
        title: k-server via multiscale entropic regularization
        journal: arXiv abs/1711.01085
        url: http://arxiv.org/abs/1711.01085
        blog: ./2017/12/20/kserver
        highlight: A breakthough in the k-server problem. The first o(k)-competitive algorithm for hierarchically separated trees.
        
      - author: Yin Tat Lee, Santosh S. Vempala
        title: Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster Polytope Volume Computation
        journal: arXiv abs/1710.06261
        url: http://arxiv.org/abs/1710.06261
        highlight: A quadratic improvement of polytope volume computation!
        
      - author: Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Akshay Ramachandran
        title: The Paulsen Problem, Continuous Operator Scaling, and Smoothed Analysis
        journal: arXiv abs/1710.02587
        url: http://arxiv.org/abs/1710.02587
        highlight: Resolving the Paulsen problem - a basic open problem in frame theory with applications from signal processing, communication theory and theoretical computer science.
  
      - author: Hongzhou Lin, Julien Mairal, Zaid Harchaoui
        title: Catalyst Acceleration for Gradient-Based Non-Convex Optimization
        journal: arXiv abs/1703.10993
        url: https://arxiv.org/abs/1703.10993
        highlight: The paper presents the extension of the Catalyst acceleration scheme for gradient <i>non-convex</i> optimization. When the objective is convex, 4WD-Catalyst enjoys the same properties as the regular Catalyst. When the objective is nonconvex, it achieves the best known convergence rate to stationary points for first-order methods.
  
